{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "289dd2ba-d357-4145-9c50-8031c4b5ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size=32\n",
    "block_size=8\n",
    "max_iters = 5000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-3\n",
    "eval_iters=200\n",
    "torch.manual_seed(1337)\n",
    "num_heads = 4\n",
    "n_embd=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46499d5c-faac-4eb1-8226-c4ba8d165103",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('input.txt', 'r',encoding = 'utf-8')\n",
    "chars=[]\n",
    "while 1:\n",
    "    char = file.read(1)\n",
    "    chars.append(char)\n",
    "    if not char:\n",
    "        break\n",
    "    # print(char)\n",
    "\n",
    "file.close()\n",
    "file = open('input.txt', 'r',encoding = 'utf-8')\n",
    "while 1:\n",
    "    char = file.read(1)\n",
    "    chars.append(char)\n",
    "    if not char:\n",
    "        break\n",
    "    # print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae1abeb-74d4-44f7-8368-586866f88ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars.append(' ')\n",
    "vocabulary_list = sorted(list(set(chars)))\n",
    "vocab_size = len(vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a056ba5-0046-490a-93dc-28d334cc0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi={}\n",
    "itos={}\n",
    "for i,c in enumerate(vocabulary_list):\n",
    "    stoi[c] = i\n",
    "    itos[i]=c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "725b21cd-6d7c-4e83-8579-efa9d9bc0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstdata=[]\n",
    "file = open('input.txt', 'r',encoding = 'utf-8')\n",
    "while 1:\n",
    "    char = file.read(1)\n",
    "    lstdata.append(stoi[char])\n",
    "    if not char:\n",
    "        break\n",
    "file = open('more.txt', 'r',encoding = 'utf-8')\n",
    "while 1:\n",
    "    char = file.read(1)\n",
    "    lstdata.append(stoi[char])\n",
    "    if not char:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b2bd9c-d759-4246-93cb-a4ea8daaa72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when context is tensor([19])----> 48\n",
      "when context is tensor([19, 48])----> 57\n",
      "when context is tensor([19, 48, 57])----> 58\n",
      "when context is tensor([19, 48, 57, 58])----> 59\n",
      "when context is tensor([19, 48, 57, 58, 59])----> 2\n",
      "when context is tensor([19, 48, 57, 58, 59,  2])----> 16\n",
      "when context is tensor([19, 48, 57, 58, 59,  2, 16])----> 48\n",
      "when context is tensor([19, 48, 57, 58, 59,  2, 16, 48])----> 59\n"
     ]
    }
   ],
   "source": [
    "tdata = torch.tensor(lstdata)\n",
    "n=int(0.9*len(tdata))\n",
    "train_data = tdata[:n]\n",
    "val_data = tdata[n:]\n",
    "\n",
    "x=train_data[:block_size]\n",
    "y=train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    print(f'when context is {x[:i+1]}----> {y[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d6d0df9-56fb-4963-8bf9-14e25b70d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([32, 8])\n",
      "tensor([[ 1,  1, 32, 44, 42, 54, 53, 43],\n",
      "        [40, 53, 43,  2, 41, 57, 44, 43],\n",
      "        [55, 60, 59, 64,  9,  1,  1, 17],\n",
      "        [53, 43,  2, 52, 44,  2, 47, 44],\n",
      "        [53, 59, 54, 53, 58,  2, 62, 48],\n",
      "        [40, 57, 53, 40, 57, 43, 48, 53],\n",
      "        [44, 57, 44,  7,  2, 41, 60, 59],\n",
      "        [42, 54, 53, 58, 44, 53, 59,  2],\n",
      "        [ 7,  1, 32, 55, 60, 57, 53,  2],\n",
      "        [40, 48, 53, 59,  7,  2, 45, 54],\n",
      "        [62, 40, 58,  2, 47, 40, 42, 50],\n",
      "        [44,  2, 48, 52, 52, 54, 43, 44],\n",
      "        [62, 58,  2, 40, 45, 59, 44, 57],\n",
      "        [ 2, 45, 54, 57, 62, 40, 57, 43],\n",
      "        [51, 51,  7,  2, 53, 54,  2, 52],\n",
      "        [33, 57, 40, 53, 48, 54,  7,  2],\n",
      "        [11,  1, 27, 40, 64,  7,  2, 48],\n",
      "        [59, 47, 44, 57,  2, 59, 54,  2],\n",
      "        [ 1, 16, 40, 52, 48, 51, 51, 54],\n",
      "        [14, 57, 44,  2, 42, 51, 40, 52],\n",
      "        [55, 40, 53, 64,  9,  1,  1, 15],\n",
      "        [44,  2, 54, 59, 47, 44, 57,  2],\n",
      "        [ 2, 45, 40, 51, 51,  9,  2, 15],\n",
      "        [54, 60,  2, 43, 54,  2, 59, 54],\n",
      "        [ 1, 31, 40, 59, 42, 51, 48, 45],\n",
      "        [47, 40, 59, 13,  1,  1, 18, 25],\n",
      "        [53, 44,  7,  1, 25, 44, 59,  2],\n",
      "        [58,  2, 64, 54, 60, 57,  2, 42],\n",
      "        [47, 44,  2, 44, 40, 57,  2, 45],\n",
      "        [40, 53, 59,  2, 64, 54, 60,  7],\n",
      "        [ 7,  2, 59, 47, 40, 59,  2, 62],\n",
      "        [ 7,  1, 36, 47, 48, 42, 47,  2]])\n",
      "torch.Size([32, 8])\n",
      "tensor([[ 1, 32, 44, 42, 54, 53, 43,  2],\n",
      "        [53, 43,  2, 41, 57, 44, 43, 12],\n",
      "        [60, 59, 64,  9,  1,  1, 17, 34],\n",
      "        [43,  2, 52, 44,  2, 47, 44, 57],\n",
      "        [59, 54, 53, 58,  2, 62, 48, 59],\n",
      "        [57, 53, 40, 57, 43, 48, 53, 44],\n",
      "        [57, 44,  7,  2, 41, 60, 59,  2],\n",
      "        [54, 53, 58, 44, 53, 59,  2, 54],\n",
      "        [ 1, 32, 55, 60, 57, 53,  2, 40],\n",
      "        [48, 53, 59,  7,  2, 45, 54, 57],\n",
      "        [40, 58,  2, 47, 40, 42, 50,  6],\n",
      "        [ 2, 48, 52, 52, 54, 43, 44, 57],\n",
      "        [58,  2, 40, 45, 59, 44, 57,  7],\n",
      "        [45, 54, 57, 62, 40, 57, 43,  2],\n",
      "        [51,  7,  2, 53, 54,  2, 52, 54],\n",
      "        [57, 40, 53, 48, 54,  7,  2, 22],\n",
      "        [ 1, 27, 40, 64,  7,  2, 48, 45],\n",
      "        [47, 44, 57,  2, 59, 54,  2, 40],\n",
      "        [16, 40, 52, 48, 51, 51, 54,  2],\n",
      "        [57, 44,  2, 42, 51, 40, 52, 54],\n",
      "        [40, 53, 64,  9,  1,  1, 15, 14],\n",
      "        [ 2, 54, 59, 47, 44, 57,  2, 53],\n",
      "        [45, 40, 51, 51,  9,  2, 15, 60],\n",
      "        [60,  2, 43, 54,  2, 59, 54,  2],\n",
      "        [31, 40, 59, 42, 51, 48, 45, 45],\n",
      "        [40, 59, 13,  1,  1, 18, 25, 15],\n",
      "        [44,  7,  1, 25, 44, 59,  2, 61],\n",
      "        [ 2, 64, 54, 60, 57,  2, 42, 54],\n",
      "        [44,  2, 44, 40, 57,  2, 45, 54],\n",
      "        [53, 59,  2, 64, 54, 60,  7,  2],\n",
      "        [ 2, 59, 47, 40, 59,  2, 62, 48],\n",
      "        [ 1, 36, 47, 48, 42, 47,  2, 42]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x= torch.stack([train_data[i:i+block_size] for i in ix])\n",
    "    y= torch.stack([train_data[i+1:i+1+block_size] for i in ix])\n",
    "    return x,y\n",
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98ab4d2c-e78e-4bbb-8324-3aedcd41808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head attention model \"\"\"\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.query = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.value = nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #B,T,h\n",
    "        q = self.query(x) #B,T,h\n",
    "        v = self.value(x) #B,T,h\n",
    "        wei = q@k.transpose(-2,-1) * (C**-0.5) #B,T,T\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        out = wei@v #B,T,h\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48d3dee0-352d-4a7d-805c-47584c2b4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.multihead = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embd,n_embd)\n",
    "    def forward(self,x):\n",
    "        x = torch.cat([h(x) for h in self.multihead],dim=-1)\n",
    "        out = self.projection(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84d5ca05-7159-43bf-87b5-8d7c937b46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_embd,4*n_embd),nn.ReLU(),nn.Linear(4*n_embd,n_embd))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e053454-a9ef-4d88-afcc-d359eccfd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//num_heads\n",
    "        self.multihead = MultiHead(num_heads,head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self,x):\n",
    "        x = x+self.multihead(self.ln1(x)) #residual path added\n",
    "        x = x+self.ffwd(self.ln2(x)) #residual path added\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "755d51e5-7977-4057-8e2b-44705dfdb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embd) #T,C\n",
    "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
    "        # self.sa_head = Head(n_embd)\n",
    "        # self.multihead = MultiHead(num_heads,n_embd//num_heads)\n",
    "        # self.ffwd = FeedForward()\n",
    "        self.blocks = nn.Sequential(Block(),Block(),Block(),Block(),nn.LayerNorm(n_embd))\n",
    "    def forward(self,idx,targets=None):\n",
    "        #idx targets shape are (B,T)\n",
    "        idx = idx.long()\n",
    "        tok_emb = self.token_embedding_table(idx) #B,T,C\n",
    "        pos_emb = self.position_embedding_table(torch.arange(block_size)) #T,C\n",
    "        # print(tok_emb.shape,pos_emb.shape)\n",
    "        x = tok_emb+pos_emb #B,T,C\n",
    "        x = self.blocks(x)\n",
    "        # x = self.multihead(x)\n",
    "        # x = self.ffwd(x)\n",
    "        # x = self.sa_head(x)\n",
    "        logits = self.lm_head(x) #B,T,vocab_size\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            loss = F.cross_entropy(logits,targets.view(B*T))\n",
    "        return logits,loss\n",
    "    def generate(self,idx,max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "            logits,loss = self(idx_cond)\n",
    "            \n",
    "            logits = logits[:,-1,:]\n",
    "            \n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,1)\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d64dc9be-390b-4d6a-ada1-71a65abbdc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2e5ed2ea-2dca-40fe-9fd1-96ab36f42ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BigramLM()\n",
    "# idx = model.generate(torch.zeros((1,1)),100).int()\n",
    "# for i in idx[0]:\n",
    "#     print(itos[i.item()])\n",
    "# ''.join([itos[i.item()] for i in idx[0]])\n",
    "\n",
    "# logit,loss=model(xb,yb)    \n",
    "# print(loss,logit.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a62c8c8f-f375-42ea-9e1e-8b18f9163717",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits,loss = model(X,Y)\n",
    "            losses[k]=loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba793863-3432-473a-9e04-e4ed27005f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logits)\n",
    "# print(logits[:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09e22ee9-5744-4c20-b751-098d8ef9cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18d25c30-cc07-40ca-9fb5-15229f5b5b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9608464241027832\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for steps in range(max_iters):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits,loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6edef6b6-35ff-4d70-859e-dbb991d9ac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" bbegisent aftroiad dlases que upoutOte!\\nlle,.\\n\\nRETRIA:\\nThath to fall 'tping;\\nO that is to brieteth eas,\\nHow slalse, Why they you do in it deat in held your theak, tal you\\nShale your renger of hy this disancly and proyders an sonoun your ince,\\nIloud here furesine feeft, Woet, blood, for a flicate but curnied hoogh of the a wharte a hearet\\nsay life in anted and to the scangely my do fill, that mast taters.\\n\\nFrom! forself angere.\\n\\nFTharch jewortool's wore themine\\nYou shall true queerss are flaist sortieple tortise a bey fake will you fure no blook'd thoughpalonse of Meing.\\n\\nDUKUCKINGBUS:\\nNom Mubt name.\\n\\nKISwivan eme dall; Lod I say as citter your bletake\\nDign:\\nYry her, bed would thee house away: sill hence.\\n\\nVORGORY Hures for\\nOf Rnab. To in this offe keice fortonon triose a thingerd:\\nAwell:\\nI made\\nI her graught thou isprame: I, telles.\\n\\nOull va, bece:\\nAing the beth though thigh\\nYou and fiinsink. Say throst and is no:. But not off, am!\\n\\nDUKE PERDY BOLIFRIA:\\nThy Marbenge feake mounmon, thy wealh your from Ruried this mpentriess Ofreat her,\\n't heress, ent in,\\nnot how to gencelow:\\nAs father veraingued affrice, my sure will I do his inear: cition'd thoust tell fortir, off Bark-\\nHust more, sure you?\\nAy, thoutime\\nSo conident,\\nTo brance your umombur upon they sime.\\n\\n\\nDUKE OF, I'll.\\nClaw, they. Bath gensen thee resedporesper what, shalll me my ofthen, and and shead wit.\\n\\nDUARDICHARforly and\\nLet willy from outh to yout to sivet and one to pridence this yours, the lode no worse 'fores you blur as\\nmowe here alt went; ogu\\nWhoath.\\n\\nLETLONIZES HENCE:\\nVare monoy disers viog becous,\\nStillies ango, whid noge my indloply with at and this their would van, lash,\\nMen at whose.\\n\\nROKINVO:\\nAnds jorter I make rive ret\\nHing the pourn?\\n\\nForbly know'll stily you my flom?\\n\\nVOBEYF LUS:\\nHow! Clolst an be old; marcaunious with well?\\n\\nGRUCKINGHERplige the man palen.\\nButere and\\nTwards I.\\n\\nThere grave beed juthing waily more to fatheriea, fall.\\n\\nBy eforist for thy afine\\nTake dio.\\nDo to as coble he hase as,\\nAnd biad Mmy whnought the ome, Canner!\\nHe yeare that\\nagave be regive'iusiss. Van yound as beath.\\n\\nHENRETHS:\\nMorrow, cout you there\\nBy all caps Here\\nHistiist\\nFour blook\\nNo sobomian,\\nGerfores how live Verydoysfied:\\nMy. Hess upos, that your no him vere of illiper\\nWhoser sie all whoes a knew he Makeng no in hy here's agaed\\nge promosed if ut noy's this lepon fir thy fathat promanful, when cocteys;manters to affey old topeft yous cut at 'take him Iself shall?\\n\\nGARD:\\nHage of your thee spird speay hen witwerd your offight I well died dartpaly seere abut, no many and to aber, no spice not offare and in and fir;\\nAuntry. I can well's\\ntelendir no vood\\nmy thiesel throw. Srut this nled accy here had ore,\\nTo froid to he everain,\\nThoEd beatty ou troke we know fortwas cere to my enerife what a dyet than\\nOre on thee I will, shen:\\nI then, and an Tre't Astource to descimite name a pake in welvorefore a man, wan:\\nTake lesperr tou lord?\\nBut say do mace.\\n\\nIshere Sight,\\nPean scame: Vrines\\nTo mortlal lodings would bekoor at bear.\\n\\nAXIDINI:\\nCyfeol.\\n\\nDUKING VIVE!\\n's shopsed would I'lvove; I beile.\\n\\nROn:\\nO state, when upone:\\nStlo I fortly to have Mastion tods\\nsene at they beat\\nHave whan. Sere me denger a meads\\nHisweld\\nAre whime thou k alon, I but I make can thould: wort to fort Righing I sorth self\\nComife jold to by hath oun come to my rifes,\\nThis wort'd times my deshere, flalsOf, my oove' sear'd theral many abus hom ounbrous you of nott aisbele love you thoughfly but, it if to benoune, knows.\\n\\nVadinstake thy spime:\\nHow. Can!\\n\\nELABELLA:\\nYounn hone this not there\\nmentle husency to houst loikews you, till amser, take both ared ait, weet lage's you sice sou duch mer theope!\\nThe peair: my ighfor terent,\\nNo were proage.\\nHe pleasces I your hepong:\\nMy grweld to daiden?ly, leat baking to Plond\\nWuriolslies and it ablay fall Mome him he shall and this blast.\\n\\nPo\\nPriap\\nO, put, thy from.\\n\\nDUKINCES:\\nAnd here he tromy lords, broke to the bitch thy soreat but bare a our grace's-phied thou asting, word forter'd,\\nWhemer, lasty?\\n\\nDuvit,\\nTrue conself she at park, her while, afty my for he nome the houst. Lage of thy it sean.\\n\\nFirch proyim:\\nShey and y'st\\nI my lapess, intriasbIts, your tal sace, I kiem!\\nYout non\\nMinty ord ylee, yout bard?\\n\\nJULIO:\\nA with there one to'ed marcy would.\\n\\nCADYREENK:\\nI liked breith toes. Theill bine your art time.\\n\\nLUPOMMBROLINk;'Lry you offor jignoast:\\nI' him not kingful noo is no thousCit, doy; ever adve you.\\nHath is letser of ctors:\\nToo stain; town.\\n\\nHENRYCIO:\\n\\nShen butomen, Bose this sogan and gewayer sied you,\\nBy it warder his try mades ring had noing of with not with at to queetorswill her, free though.\\nWhieng of at mpengelong are yelcuse? what wort.\\nRen I I'll humm\\nOl if upfend wife heur you somer ogeds; ost Yeling;'s Tries\\nHeid's me your clone.\\nFrused to hy not, o they werith diers,\\n'mur, picul andy, heir viol there didsay agasts, But?\\nChumeh, him, creiter for sime: a tounged and fortooes, Dever, of oan sfore'!\\n\\nAUSTER:\\nHe willt:\\nThouse pright of that, refaiet.\\n\\nBUCHAUKE Of Pinkined here upure through feels\\nAit he wour I dreied thee keam.\\nBe, for good.\\nMy par quiers notse the aster tacke would your man! and, hear edsild to in Fursecry nome common.\\n\\nMENENES:\\nAnd anglood to my fry,\\nTill them lapon, the kiys this choal.\\nBut thout fer if say like\\nThat your looks\\nDo the have stonva,--\\n\\nAzsty will I main of.\\n\\nTith line timedys, tabake lisedise tear and Ofer, your this in lift to you will.\\n\\nDUKE Ment with spolut were and their too.\\nWarr, eving\\nWake your Rcraien?\\n\\nLhtvy not, then I in if God so ground.\\n\\nLork.\\nO, olk. Pard Comphriustly\\nWhere should good not , my all peless house at to fry's.\\n'kings in Yout it\\nI'lest him, thoughs, Angely. Vinger,\\nAm consenglar housmy yage will sompers. God Aform allowbrear him handemer hadener'd I parlasose: not I doo,\\nMerian\\nThe I my And not liest.\\n\\nFurtyfore, lief.\\n\\nCy well'd he, bewith my wrotheng:\\nBut murijeancy and hard forthi,\\nI your'd stap.\\n\\nSet than I strect\\nBine's Let corly, he\\nTurt, my clove not cits;\\nHer?\\n\\nComan conew.\\n\\nFeretture macke your this this you till:\\nAllow;\\nPo.\\n\\nThat the grais,\\nAnd goddeny, oold morak.\\n\\nPEYOUTUL:\\nNelf?'Themens,\\nProth. He my the incauster.\\n\\nSerfidy give\\nloygus; cavius?\\n\\nMARD PANI:\\nHad your with upince love\\nAnd by to by dear.\\n\\nLADY Ogeliner\\nBut bele ands do's wheen more, his aftiendan owizest and\\nHow, Angely they busong?\\n\\nMARGESMRUT:\\nWith thou foll wet bete! as cloved:\\nHarforedersabunce,\\nOu.\\nMy.\\n\\nGy putels Kesfore not\\nAnd ive well furtor:\\nThouse him,\\nYou than theee be cunfor;\\nLord come as than bly word surving as then ait bere rept love,\\nAnd as will and of of Paswere,\\nFirst this is\\naf preplay, from But enter\\nTull merow foor hame am;\\nSolistime you tal he wands witither your, antines ague.\\n\\nPOLEOS:\\nBe you shaps it Gady morrod tisios ried your read kingself my\\nTrethere, but my amel's I of him Aneed will proitchs\\nthe Mysear haw thif wroth. I deness yourr ware art thee new in the biting to stied in in you routh no so foll of otur and offrand 'an.\\n\\njor, as him offater, one thouse.\\nMistagen Right\\nMoow to mun? To:\\nWhat there damanime,\\nAs citlandmans as wrace: Hinkneal thely will rewell all their me. Gell is the here my man a yearm so rays,\\nAnd my chander pay an blowive this,\\nHrow.\\nVI him.\\n\\nMy whis boing dishad youldangion thear\\ndut you, wharde to is joldayty is for thy what,\\nI 'Git!\\n\\nAusis by to that be dediesbuth oaty.\\n\\nGREZF I'll you, this yruch are.\\n\\nAULEERS:\\nProychfeargart new miders in marrom nour beath my has he astines 'tlioss vess ben thy to these fall bear; bentonous violliembore wat.\\n\\nGARD:\\nNorth was mee\\nThen fair dimphere in your plood,--\\nHis flastay? mere Pout.\\n\\nCADWIRLes and bear'There's onemy roze they wrocenough whack:\\nO, say so olmy. Nost gives fear tong that spity. You looke's there, prour wull's evere of Rome:\\nThe drasine,\\nTo with grue:\\nAs of munemy mooly, in Horsendy, when; Mere so sposort their, of Membloes theif\\n\\nPUETRUCHARD:\\nI welp, I wor lose knoys, veat, thee proy kneze,\\nDo daster here that osblove.\\nNording.\\nMeforiet, blone saglo; his\\nmore so sour heiw theal hich fork, his for loods\\nthou?\\nAd after.\\n\\nGURETH:\\nFirscour'd pecoter and sausely they,\\nAn-iline, I scohpison they proke' his but stry.\\n\\nGLEONTIO:\\nProight, you,\\nsay, give to abe houtide. And no take as have me as abunter\\nI shas thou\\nhand ace it beak now, so same very his anful, wrucoty youser he save the lal\\nBut priess? which plows lay this begs: agapt peay.\\n\\nPropt to giugn wash these's oftle dormer, love shall the can turcubburn of that which he kine too, mone foot shall, andy for astelf.\\n\\nFirenty you singer'Talisty:\\nForthies, of. Come your him: is you,\\nI grow you that the.\\nGers, delk yrusofort man: that but to piiat in frith if homesigh: how take my grom, crusure'ery Jyou: your unmy, heire:\\nO hijund thy at  fill enands to coused;\\nAnd slow'ill, a wromes yous as to penees, I qood my grid and death time; love to and oner a it.\\n\\nCrail?\\nHity gut mine,\\n'Rnot\\nMy to iThart, Pa, him, he rethough soon\\na sof, his to lenfisty she svice,\\nHe, Pold,\\nIt whime me to use and lones and give swilt.\\n\\nBRUCORIND:\\nBut my greaviod eake mave that you bitien: his drump tentere's no lort viited:\\nThere to a know wall fam here seep full would so,e anding in hever be twees, I love you: lord: freakins.\\n\\nELAMBELLANUS:\\nSigetius:\\nTo do in while; from I muty aere shas theme: mysqell,\\nAnd string old you this have will ofwift him you, farains, to for my you dieng.\\nBun: my was werer: I wouldem?\\nSee men cont say, prom hee quary there a hand.\\nHy prokson ilstege and off younter warr 'st pince and.\\n\\nFRUTRIO!-HAMSSTBF I hise cade self theirgp bave tram, her witure viedder-fare:es hard verse Heir.\\nThere, buchere.\\nHave wip, and after's this I phy.\\n\\nMICHARGS:\\nI which queopt, hefeep an Preng I mine:\\nMy coned the drow and boort heir now words.\\n\\nDive mine to your your gent, and come. Garke him, my not cotaze mine the come him pilius;\\nOn, of no remy thand conself\\nWhought raigh,\\nWerpickyme wlond, creas unt home.\\n\\nMENENTET:\\nGly afpeert it\\nhast not thy his to can thim tonouthau not\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = model.generate(torch.zeros((1,block_size)),10000).int()\n",
    "''.join([itos[i.item()] for i in idx[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea970d9-d28b-48aa-b5d5-57ea5732e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nn.Embedding(vocab_size,vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88188833-a514-4237-a72b-da3af98faa53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6056, -1.3920,  1.0248,  ...,  0.3656, -1.7355,  0.2139],\n",
       "        [ 0.8255, -2.6329, -0.9479,  ...,  0.6256,  0.0523,  0.9869],\n",
       "        [ 0.7193,  0.5352, -0.6094,  ...,  2.0635,  0.7135, -0.6963],\n",
       "        ...,\n",
       "        [ 0.5466,  0.5238, -1.3927,  ..., -1.0646, -1.0925, -2.9220],\n",
       "        [-1.9872,  0.6649, -0.4610,  ..., -2.5498, -0.9464,  0.0490],\n",
       "        [-2.0957, -0.2855, -1.2304,  ..., -0.1399, -1.2145,  0.1204]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b840e7a8-aedf-4f78-84a4-77f38ed31564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6056, -1.3920,  1.0248, -0.5932,  0.0104,  0.5026, -1.0102,  0.3182,\n",
       "          0.5429, -0.8440,  1.6144, -1.2571, -0.7955, -0.8091,  0.3076,  0.3817,\n",
       "         -1.4066, -0.4319, -0.5875, -0.7718, -0.3235, -0.1856,  0.8806,  2.4690,\n",
       "          0.6213,  0.5952, -0.3065,  0.0720, -0.4091,  0.1777, -0.7487,  1.8724,\n",
       "         -1.2306,  0.6257, -1.3147, -0.6245, -0.2754,  1.3851, -0.2040,  0.7382,\n",
       "          1.0386, -0.2191, -2.0600, -0.6788, -0.4363, -1.8781, -0.1640,  0.4619,\n",
       "          0.1324, -0.1182, -0.5386,  0.4479,  0.4336,  1.2843,  0.0249,  0.5398,\n",
       "         -2.2262, -0.7744, -0.6222, -0.1747,  0.6970, -0.3712, -1.0101,  0.3656,\n",
       "         -1.7355,  0.2139]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.tensor([0]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4764f034-0e6f-4362-90af-a7e5a37baf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1866, -0.1730],\n",
      "        [ 0.8760, -2.0884],\n",
      "        [ 1.3665,  0.7407],\n",
      "        [-0.8617, -1.4160],\n",
      "        [-1.3838, -1.1819],\n",
      "        [ 1.2904, -0.7376],\n",
      "        [ 0.0343,  2.1095],\n",
      "        [ 1.7741,  0.7654]])\n",
      "tensor([[-0.1866, -0.1730],\n",
      "        [ 0.3447, -1.1307],\n",
      "        [ 0.6853, -0.5069],\n",
      "        [ 0.2986, -0.7342],\n",
      "        [-0.0379, -0.8237],\n",
      "        [ 0.1835, -0.8093],\n",
      "        [ 0.1622, -0.3924],\n",
      "        [ 0.3637, -0.2477]])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "tdata = torch.randn(B,T,C)\n",
    "ctx = torch.zeros(B,T,C)\n",
    "tdata[0,:1,:].mean(dim=0)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        prev = tdata[b,:t+1,:]\n",
    "        ctx[b,t,:] = prev.mean(dim=0)\n",
    "\n",
    "print(tdata[0])\n",
    "print(ctx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0db8820-648e-4924-9ffe-3608c6b989ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
       "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
       "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "head_size=16\n",
    "x = torch.randn((B,T,C))\n",
    "ql = nn.Linear(C,head_size,bias=False)\n",
    "kl = nn.Linear(C,head_size,bias=False)\n",
    "vl = nn.Linear(C,head_size,bias=False)\n",
    "q = ql(x) #B,T,h\n",
    "k = kl(x) #B,T,h\n",
    "v = vl(x)\n",
    "qk = q@k.transpose(-2,-1) #B,T,16 @ B,16,T---->B,T,T\n",
    "b = torch.ones(T,T)\n",
    "tril = torch.tril(b)\n",
    "wei = qk\n",
    "wei = wei.masked_fill(tril==0,float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "out = wei@v\n",
    "out.shape\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "746709e1-3295-4140-a86c-4bac7d5127e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1,block_size))[:,-block_size:]\n",
    "torch.arange(block_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
